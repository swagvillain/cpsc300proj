{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1k4NmzAWTKy"
   },
   "source": [
    "# COMFE\n",
    "CPSC 300 Software Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "XEYL8MqtVw1y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, recall_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRFi6QvybKuJ"
   },
   "source": [
    "## Data Set Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehy1PX88ZvKT",
    "outputId": "2c23fa67-a037-4ade-f2ec-e2da71c3d507"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_with_dist_to_near1.csv\")\n",
    "# data = pd.read_csv(\"C:\\\\Users\\\\swagvillain\\\\Downloads\\\\bounding_boxes_with_labels.csv\")\n",
    "\n",
    "\n",
    "# Displaying the first few rows of the data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0N3OWajymzI",
    "outputId": "cc355070-c49d-454f-e2f9-b191ea820ab9"
   },
   "outputs": [],
   "source": [
    "# Summary of data\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "8-h4DpuKyonX",
    "outputId": "16ce1876-3a7d-452d-be72-0deee8383435"
   },
   "outputs": [],
   "source": [
    "# Description of data\n",
    "data.describe(include=\"all\", percentiles=([0.1, 0.25, 0.5, 0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding rows with co-efficients 0 (flat objects)\n",
    "Trying to find walls, floors etc. that we may not want to consider in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where any of the specified columns have a value less than 1.0\n",
    "filtered_rows = data[(data['coeffs_x'] <= 1.0) | (data['coeffs_y'] <= 1.0) | (data['coeffs_z'] <= 1.0) ]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(filtered_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCeMUmTpb0zR"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O1pD4NG3xSLL",
    "outputId": "ae691eac-ebf6-4289-8a96-c9558792bd26"
   },
   "outputs": [],
   "source": [
    "# finding how many duplicate rows we have\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rounding (already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def round_near_integers(data, threshold=1e-1):\n",
    "#     \"\"\"\n",
    "#     Round values close to 0, 1, or -1 in a DataFrame to their nearest integers.\n",
    "\n",
    "#     Parameters:\n",
    "#         data (pd.DataFrame): The input DataFrame.\n",
    "#         threshold (float): The tolerance for rounding near integers.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: The modified DataFrame with rounded values.\n",
    "#     \"\"\"\n",
    "#     # Define a rounding function\n",
    "#     def round_values(value):\n",
    "#         if np.isclose(value, 0, atol=threshold):\n",
    "#             return 0\n",
    "#         elif np.isclose(value, 1, atol=threshold):\n",
    "#             return 1\n",
    "#         elif np.isclose(value, -1, atol=threshold):\n",
    "#             return -1\n",
    "#         else:\n",
    "#             return value\n",
    "\n",
    "#     # Identify numeric columns to round, excluding 'ID' and 'room_ID'\n",
    "#     numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "#     columns_to_round = [col for col in numeric_columns if col not in ['ID', 'room_ID']]\n",
    "    \n",
    "#     # Apply rounding function to the selected columns\n",
    "#     data[columns_to_round] = data[columns_to_round].applymap(round_values)\n",
    "#     return data\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMtKGUjry4_0",
    "outputId": "cdcb7f9a-55a8-4662-9c35-4c64a25d5230"
   },
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6bBWHjL0oP-"
   },
   "source": [
    "### Correlations and visualization\n",
    "\n",
    "Let's look at the data correlations. first we split the data into numeric and categorical, then we will look at correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1IN2XzM0sm-"
   },
   "outputs": [],
   "source": [
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "categorical_data = data.select_dtypes(exclude=[np.number])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "RWj0m6vg1fEx",
    "outputId": "d516a25e-121e-4bb5-f737-e8e752e318cd"
   },
   "outputs": [],
   "source": [
    "corr = numeric_data.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3ZFnLczJSvJ"
   },
   "source": [
    "## Modelling Time.\n",
    "Let's make some models to train on our cleaned and processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating nearest other bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Assuming `data` is the DataFrame provided\n",
    "# data['dist_to_nearest'] = np.inf  # Initialize with infinity for all rows\n",
    "\n",
    "# # Function to calculate Euclidean distance between two bounding boxes\n",
    "# def compute_distance(row1, row2, dimensions=['x', 'y', 'z']):\n",
    "#     deltas = []\n",
    "#     for dim in dimensions:\n",
    "#         delta = (row1[f'centroid_{dim}'] - row2[f'centroid_{dim}']) ** 2\n",
    "#         deltas.append(delta)\n",
    "#     return np.sqrt(sum(deltas))\n",
    "\n",
    "# # Iterate over each unique room_ID\n",
    "# for room_id in tqdm(data['room_ID'].unique(), desc=\"Processing room_IDs\"):\n",
    "#     room_data = data[data['room_ID'] == room_id]\n",
    "    \n",
    "#     # Iterate over each bounding box in the room\n",
    "#     for i, row in room_data.iterrows():\n",
    "#         min_distance = np.inf\n",
    "#         for j, other_row in room_data.iterrows():\n",
    "#             if i != j:  # Skip the current box itself\n",
    "#                 dist = compute_distance(row, other_row, dimensions=['x', 'y', 'z'])\n",
    "#                 min_distance = min(min_distance, dist)\n",
    "        \n",
    "#         # Update the distance for this row in the main DataFrame\n",
    "#         data.at[i, 'dist_to_nearest'] = min_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head)\n",
    "\n",
    "data.to_csv('data_with_dist_to_near1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding fake data to the mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove basis from real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Unnamed: 0.1','Unnamed: 0','basis_0_0', 'basis_0_0', 'basis_0_1', 'basis_0_2', 'basis_1_0', 'basis_1_1', 'basis_1_2', 'basis_2_0',  'basis_2_1',  'basis_2_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of rows\n",
    "num_rows = 200000\n",
    "\n",
    "# Generate random data based on the provided statistics\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "dummy_data = {\n",
    "    # \"basis_0_0\": np.random.randint(-1, 1, size=num_rows), ignore the rotation\n",
    "    # \"basis_0_1\": np.random.randint(-1, 1, size=num_rows),\n",
    "    # \"basis_0_2\": np.random.randint(-1, 1, size=num_rows),\n",
    "    # \"basis_1_0\": np.random.randint(-1, 1, size=num_rows),\n",
    "    # \"basis_1_1\": np.random.randint(-1, 1, size=num_rows),\n",
    "    # \"basis_1_2\": np.random.randint(-1, 1, size=num_rows),\n",
    "    # \"basis_2_0\": np.random.randint(-1, 1, size=num_rows),\n",
    "    # \"basis_2_1\": np.random.randint(-1, 1, size=num_rows),\n",
    "    # \"basis_2_2\": np.random.randint(-1, 1, size=num_rows),\n",
    "    \"centroid_x\": np.random.normal(-39345, 2000, size=num_rows),  # mean=39345, std=9316\n",
    "    \"centroid_y\": np.random.uniform(-44081, 1000, size=num_rows), # mean=-44081, std=6586\n",
    "    \"centroid_z\": np.random.normal(-3873, 22531, size=num_rows),\n",
    "    \"coeffs_x\": np.random.uniform(0, 19234, size=num_rows),\n",
    "    \"coeffs_y\": np.random.uniform(0, 17126, size=num_rows),\n",
    "    \"coeffs_z\": np.random.uniform(0, 7747, size=num_rows),\n",
    "    \"room_ID\": np.random.randint(0, 3496, size=num_rows),\n",
    "    \"dist_to_nearest\": np.random.exponential(scale=734.26, size=num_rows),  # Using mean to set scale\n",
    "    \"is_real\": 0\n",
    "}\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "dummy_data = pd.DataFrame(dummy_data)\n",
    "\n",
    "# Display the first few rows\n",
    "print(dummy_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add column to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_real'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([data, dummy_data], ignore_index=True)\n",
    "\n",
    "print(full_data.head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlO4WB3IJocQ"
   },
   "source": [
    "### splitting data\n",
    "We split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8u0EtYMsJseZ",
    "outputId": "d4134be2-3456-4fbb-8ee5-73e0e939a9a9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = full_data.drop(columns=['is_real'])\n",
    "y = full_data['is_real']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# checking to see if it was split correctly\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdbx1EDNQhnR"
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRVeiQHYkAte"
   },
   "source": [
    "making tree classifier and using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_7JPUUCQkg3",
    "outputId": "1cf96c33-db54-43d0-afc9-5922d4856cc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-Validation Progress: 100%|██████████| 5/5 [00:07<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy (Decision Tree): 0.9999883639560607\n",
      "Standard Deviation Accuracy (Decision Tree): 3.878665642114124e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Create Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=5, min_samples_split=10)\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Initialize progress bar\n",
    "progress_bar = tqdm(total=cv.get_n_splits(X_train, y_train), desc=\"Cross-Validation Progress\")\n",
    "\n",
    "# Perform cross-validation manually to show progress\n",
    "dt_scores = []\n",
    "for train_idx, test_idx in cv.split(X_train, y_train):\n",
    "    X_fold_train, X_fold_test = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "    y_fold_train, y_fold_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "    dt_classifier.fit(X_fold_train, y_fold_train)\n",
    "    dt_scores.append(dt_classifier.score(X_fold_test, y_fold_test))\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "# Convert scores to a NumPy array for mean and standard deviation calculations\n",
    "dt_scores = np.array(dt_scores)\n",
    "\n",
    "# Mean accuracy across folds\n",
    "print(\"Mean Accuracy (Decision Tree):\", dt_scores.mean())\n",
    "print(\"Standard Deviation Accuracy (Decision Tree):\", dt_scores.std())\n",
    "\n",
    "# Exporting/saving the model\n",
    "\n",
    "# joblib.dump(dt_classifier, 'COMFE_model.pkl')\n",
    "with open('COMFE_model.pkl', 'wb') as f:\n",
    "    pickle.dump(dt_classifier, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HwfUIV8lFG-"
   },
   "source": [
    "#### Finding Best Hyperparameters for DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9TsPSGbiSbJ",
    "outputId": "524e1b0c-4f0b-4c37-8dae-f8f1c2fe2bd9"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define hyperparameters grid for Decision Tree\n",
    "# dt_param_grid = {\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "# }\n",
    "\n",
    "# # GridSearchCV for Decision Tree\n",
    "# dt_grid_search = GridSearchCV(DecisionTreeClassifier(), dt_param_grid, cv=5)\n",
    "# dt_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters for Decision Tree\n",
    "# print(\"Best Parameters (Decision Tree):\", dt_grid_search.best_params_)\n",
    "# print(\"Best Score (Decision Tree):\", dt_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2TPn7hhibJo"
   },
   "source": [
    "we see that the best max_depth is 10. We will use that below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rLMDLJ_lPTY"
   },
   "source": [
    "#### making final (best) decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zURlzBFajaK4",
    "outputId": "ecfbd135-46fc-4be1-8a1d-e05570ce95df"
   },
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# # Define the Decision Tree classifier with the best parameters\n",
    "# best_dt_classifier = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "# # Train the model on the full training set\n",
    "# best_dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model on the testing set\n",
    "# dt_test_accuracy = best_dt_classifier.score(X_test, y_test)\n",
    "# print(\"Test Accuracy (Decision Tree):\", dt_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyBWLKuRQpiS"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFCuSrRPnC4f"
   },
   "source": [
    "#### making the model and using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.isnull().sum())  # Shows count of NaNs per feature\n",
    "# print(y_train.isnull().sum())  # Check for NaNs in target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqWmLszqmUnL",
    "outputId": "3314f6c7-3d72-49df-e782-636737835e96"
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Scale the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# # Create Logistic Regression classifier with adjusted parameters\n",
    "# logreg_classifier = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# # Use StratifiedKFold for cross-validation\n",
    "# stratified_cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# # Perform cross-validation for Logistic Regression classifier\n",
    "# logreg_scores = cross_val_score(logreg_classifier, X_train_scaled, y_train, cv=stratified_cv)\n",
    "\n",
    "# # Print the cross-validation scores\n",
    "# print(\"Cross-Validation Scores (Logistic Regression):\", logreg_scores)\n",
    "\n",
    "# # Print the mean and standard deviation of the cross-validation scores\n",
    "# print(\"Mean Accuracy (Logistic Regression):\", logreg_scores.mean())\n",
    "# print(\"Standard Deviation of Accuracy (Logistic Regression):\", logreg_scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlucQb0kaTVC"
   },
   "source": [
    "### Finding most important features in the decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "a_ok2musaSQn",
    "outputId": "67fec2a6-f6e7-4967-95e5-bb272d5dbc35"
   },
   "outputs": [],
   "source": [
    "feature_importances = best_dt_classifier.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a dictionary mapping feature names to their importances\n",
    "feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "\n",
    "# Sort features by their importances\n",
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the most important features\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "for feature, importance in sorted_features[:5]:\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_features[:10])), [imp for _, imp in sorted_features[:10]], align='center')\n",
    "plt.yticks(range(len(sorted_features[:10])), [feat for feat, _ in sorted_features[:10]])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
